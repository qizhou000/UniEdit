edit_model_name: "llama-3-8b"
begin_layer_path: "model.layers.0"
lm_head_path: "lm_head"
max_demonstration_n: 32
max_demo_tokens: 400
