edit_model_name: "llama-3-8b"
edit_layer: 31
num_steps: 75
lr: 1.e-2
loss_a_lambda: 1.e-1
loss_m_lambda: 1.e-1
weight_decay: 0
mlp_in_module_tmps: 
- "model.layers.{}.mlp.gate_proj"
- "model.layers.{}.mlp.up_proj"
mlp_out_module_tmps: 
- "model.layers.{}.mlp.down_proj"
